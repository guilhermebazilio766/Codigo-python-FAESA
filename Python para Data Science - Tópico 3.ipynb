
Respostas a perguntas pendentes
Q: Como proteger meu código Python?
R: Código Python é particularmente vulnerável e esta linguagem pode não ser a mais adequada quando é necessário distribuir o programa sem permitir acesso ao código fonte. Alguns modos de lidar com isso são:
"Compilar" seu código para um bytecode com formato .pyc
Pode ser descompilado, então não resolve o problema definitivamente
Pelo prompt anaconda:
python -m py_compile programa.py
python -m compileall programa.py
Pelo próprio Python:
import my_compile; py_compile.compile("programa.py")
O arquivo "compilado" se encontrará na pasta __pycache__
Convertendo seu código em um arquivo executável
Duas ferramentas disponíveis são a pyinstaller e a py2exe
O que elas fazem é, basicamente, converter seu código para .pyc e compactar em um arquivo executável que executa um python portátil que, por sua vez, executa seu código.
Ou seja, também não resolve o problema definitivamente
Distribuindo seu software como um serviço
Você pode contratar uma plataforma de SaaS (Software as a Service) e vender o acesso ao serviço, não o programa em si.
Este método, sim, pode ser considerada uma forma mais robusta de esconder seu código. Porém, tem seus custos.
Contratos com termos e condições de uso
Notem que, no contexto de análise de dados, nada disso costuma ser necessário
Q: Como carregar um arquivo parcialmente, sem manter ele completo na memória?
R: O método read_csv do Pandas permite o uso do parâmetro opcional chunksize, que recebe o número de linhas a serem lidas por vez.
Quando utilizado, o método retorna um objeto do tipo pandas.io.parsers.TextFileReader
Ao iterar este objeto, a cada iteração recebemos um DataFrame
for pedaço in pd.read_csv("arquivo.csv", chunksize=10): print(pedaço)
Q: Alguma matéria irá cobrir outras bibliotecas do Python?
R: O próximo módulo já é o de machine learning! Em breve o professor Howard estará liberando a ementa da matéria no AVA.
Matplotlib
In [101]:
import matplotlib.pyplot as plt
In [102]:
coleção = [1,2,4,8,16]

plt.plot(coleção)
Out[102]:
[<matplotlib.lines.Line2D at 0x2b4a2169700>]

In [103]:
plt.pie(coleção)
Out[103]:
([<matplotlib.patches.Wedge at 0x2b4a21cd580>,
  <matplotlib.patches.Wedge at 0x2b4a21cda60>,
  <matplotlib.patches.Wedge at 0x2b4a21cdee0>,
  <matplotlib.patches.Wedge at 0x2b4a21da3a0>,
  <matplotlib.patches.Wedge at 0x2b4a21da850>],
 [Text(1.0943562560671902, 0.11128515088098098, ''),
  Text(1.0108535980228326, 0.4337914284126577, ''),
  Text(0.5818604395536556, 0.933508665670773, ''),
  Text(-0.6733165230052025, 0.869853355369849, ''),
  Text(-0.055714238349465386, -1.098588150147788, '')])

In [104]:
plt.hist(coleção)
Out[104]:
(array([2., 0., 1., 0., 1., 0., 0., 0., 0., 1.]),
 array([ 1. ,  2.5,  4. ,  5.5,  7. ,  8.5, 10. , 11.5, 13. , 14.5, 16. ]),
 <BarContainer object of 10 artists>)

In [105]:
X = [1,3,5,9,12]
Y = [8,5,2,6,1]

plt.plot(X,Y)
Out[105]:
[<matplotlib.lines.Line2D at 0x2b4a2292e50>]

In [106]:
plt.scatter(X,Y)
Out[106]:
<matplotlib.collections.PathCollection at 0x2b4a22efbb0>

In [107]:
plt.bar(X,Y)
Out[107]:
<BarContainer object of 5 artists>

In [108]:
plt.barh(X,Y)
Out[108]:
<BarContainer object of 5 artists>

In [112]:
X_texto = ["altura 1", "altura 2", "altura 3", "altura 4", "altura 5"]
In [113]:
plt.bar(X_texto,Y)
Out[113]:
<BarContainer object of 5 artists>

In [114]:
plt.barh(X_texto,Y)
Out[114]:
<BarContainer object of 5 artists>

Personalizando
In [115]:
coleção = [1,2,4,8,16]

plt.plot(coleção,"ko--")
Out[115]:
[<matplotlib.lines.Line2D at 0x2b4a098c0d0>]

In [117]:
coleção = [1,2,4,8,16]

plt.plot(coleção,color="k",linestyle="dashed",marker="o",linewidth=2)
Out[117]:
[<matplotlib.lines.Line2D at 0x2b4a1faf130>]

In [119]:
coleção = [1,2,4,8,16]

plt.plot(coleção,color="#631e58",linestyle="dashed",marker="o",linewidth=2)
Out[119]:
[<matplotlib.lines.Line2D at 0x2b4a36643a0>]

In [124]:
coleção = [1,2,4,8,16]

plt.plot(coleção,color="#631e58",linestyle="dashed",marker="o",linewidth=2,zorder=2,label="linha")

X = [1,3,5,9,12]
Y = [8,5,2,6,1]

plt.bar(X,Y,zorder=1,label="barras")
Out[124]:
<BarContainer object of 5 artists>

Subplots e mais opções de personalização
novo plot sempre é adicionado ao último subplot criado
In [144]:
fig = plt.figure(figsize=(5,5),dpi=100)
a = fig.add_subplot(2,2,1)

coleção = [1,2,4,8,16]
plt.plot(coleção,color="#631e58",linestyle="dashed",marker="o",linewidth=2,zorder=2,label="linha")
X = [1,3,5,9,12]
Y = [8,5,2,6,1]
plt.bar(X,Y,zorder=1,label="barras")

a.legend()

a = fig.add_subplot(2,2,2)
plt.pie(coleção)

a = fig.add_subplot(2,2,3)
plt.bar(X_texto,Y)

a = fig.add_subplot(2,2,4)
plt.barh(X_texto,Y)
Out[144]:
<BarContainer object of 5 artists>

In [168]:
fig = plt.figure(figsize=(15,15),dpi=100)
a = fig.add_subplot(2,2,1)

coleção = [1,2,4,8,16]
plt.plot(coleção,color="#631e58",linestyle="dashed",marker="o",linewidth=2,zorder=2,label="linha",rasterized=True)
plt.plot(Y,zorder=1,label="linha2",rasterized=True)
#plt.xlim([0,10])
plt.ylim([0,20])
a.set_xticks([0,3,6,9])
a.set_xticklabels(["começo","meio","quase lá","acabou"],
                 rotation=30,fontsize="small")
a.set_title("Primeiro gráfico")
a.set_xlabel("Eixo horizontal")
a.set_ylabel("Eixo vertical")

a.legend()

a = fig.add_subplot(2,2,2)
plt.pie(coleção)
a.set_title("Segundo gráfico")

a = fig.add_subplot(2,2,3)
plt.bar([1,2,3,4,5],Y)
a.set_xticks([1,2,3,4,5])
a.set_xticklabels(X_texto, rotation=-30,fontsize="small")


a = fig.add_subplot(2,2,4)
plt.barh(X_texto,Y)

plt.subplots_adjust(wspace=0.5,hspace=0.5)

#plt.savefig("Gráfico.png",transparent=True)
plt.savefig("Gráfico.pdf",transparent=True)

Usando um dataset real
In [420]:
import pandas as pd

df = pd.read_csv("cases-brazil-cities-time.csv")
In [421]:
df
Out[421]:
epi_week	date	country	state	city	ibgeID	cod_RegiaoDeSaude	name_RegiaoDeSaude	newDeaths	deaths	newCases	totalCases	deaths_per_100k_inhabitants	totalCases_per_100k_inhabitants	deaths_by_totalCases	_source	last_info_date
0	9	2020-02-25	Brazil	SP	São Paulo/SP	3550308	35016.0	São Paulo	0	0	1	1	0.00000	0.00811	0.00000	SES	2021-05-24
1	9	2020-02-25	Brazil	TOTAL	TOTAL	0	NaN	NaN	0	0	1	1	0.00000	0.00047	0.00000	NaN	NaN
2	9	2020-02-26	Brazil	SP	São Paulo/SP	3550308	35016.0	São Paulo	0	0	0	1	0.00000	0.00811	0.00000	SES	2021-05-24
3	9	2020-02-26	Brazil	TOTAL	TOTAL	0	NaN	NaN	0	0	0	1	0.00000	0.00047	0.00000	NaN	NaN
4	9	2020-02-27	Brazil	SP	São Paulo/SP	3550308	35016.0	São Paulo	0	0	0	1	0.00000	0.00811	0.00000	SES	2021-05-24
...	...	...	...	...	...	...	...	...	...	...	...	...	...	...	...	...	...
2131457	121	2021-05-26	Brazil	PR	Ângulo/PR	4101150	41015.0	15ª RS Maringá	0	6	0	283	204.77816	9658.70307	0.02120	SES	2021-05-26
2131458	121	2021-05-26	Brazil	BA	Érico Cardoso/BA	2900504	29003.0	Brumado	0	2	5	240	18.93939	2272.72727	0.00833	MS	2021-05-26
2131459	121	2021-05-26	Brazil	PA	Óbidos/PA	1505106	15002.0	Baixo Amazonas	0	116	0	6062	221.77188	11589.49260	0.01914	MS	2021-05-26
2131460	121	2021-05-26	Brazil	SP	Óleo/SP	3533809	35094.0	Ourinhos	0	1	1	71	40.46945	2873.33064	0.01408	MS	2021-05-26
2131461	121	2021-05-26	Brazil	TOTAL	TOTAL	0	NaN	NaN	2334	454768	75087	16280392	214.76070	7688.29015	0.02793	NaN	NaN
2131462 rows × 17 columns

In [ ]:
df_sp = df.loc[df.city == "São Paulo/SP"]

plt.plot(df_sp.date,df_sp.deaths)
Out[ ]:
[<matplotlib.lines.Line2D at 0x2b4ce2ac3a0>]
In [173]:
df_sp.date[::50]
Out[173]:
0          2020-02-25
15559      2020-04-15
160585     2020-06-04
419776     2020-07-24
698368     2020-09-12
978104     2020-11-01
1257904    2020-12-21
1537704    2021-02-09
1817504    2021-03-31
2097304    2021-05-20
Name: date, dtype: object
In [183]:
fig2 = plt.figure(figsize=(10,10))

a = fig2.add_subplot(1,1,1)

df_sp = df.loc[df.city == "São Paulo/SP"]

plt.plot(df_sp.date,df_sp.deaths)

df_rj = df.loc[df.city == "Rio de Janeiro/RJ"]

plt.plot(df_rj.date,df_rj.deaths,color="red")

a.set_xticks(df_sp.date[::25])
a.set_xticklabels(df_sp.date[::25],rotation=30,fontsize="small")
Out[183]:
[Text(0.0, 0, '2020-02-25'),
 Text(25.0, 0, '2020-03-21'),
 Text(50.0, 0, '2020-04-15'),
 Text(75.0, 0, '2020-05-10'),
 Text(100.0, 0, '2020-06-04'),
 Text(125.0, 0, '2020-06-29'),
 Text(150.0, 0, '2020-07-24'),
 Text(175.0, 0, '2020-08-18'),
 Text(200.0, 0, '2020-09-12'),
 Text(225.0, 0, '2020-10-07'),
 Text(250.0, 0, '2020-11-01'),
 Text(275.0, 0, '2020-11-26'),
 Text(300.0, 0, '2020-12-21'),
 Text(325.0, 0, '2021-01-15'),
 Text(350.0, 0, '2021-02-09'),
 Text(375.0, 0, '2021-03-06'),
 Text(400.0, 0, '2021-03-31'),
 Text(425.0, 0, '2021-04-25'),
 Text(450.0, 0, '2021-05-20')]

SciKit
Pré-processar
Limpeza
Valor ausente
Valor inconsistente
Ruído
Integrar
Seleção
Transformação
Aprendizado
Supervisionado
Treinamento com respostas acerca de um conjunto de treino
Fase de treino e fase de testes
Toy datasets
Não supervisionado
In [184]:
from sklearn import datasets

dados = datasets.load_breast_cancer()

dados
Out[184]:
{'data': array([[1.799e+01, 1.038e+01, 1.228e+02, ..., 2.654e-01, 4.601e-01,
         1.189e-01],
        [2.057e+01, 1.777e+01, 1.329e+02, ..., 1.860e-01, 2.750e-01,
         8.902e-02],
        [1.969e+01, 2.125e+01, 1.300e+02, ..., 2.430e-01, 3.613e-01,
         8.758e-02],
        ...,
        [1.660e+01, 2.808e+01, 1.083e+02, ..., 1.418e-01, 2.218e-01,
         7.820e-02],
        [2.060e+01, 2.933e+01, 1.401e+02, ..., 2.650e-01, 4.087e-01,
         1.240e-01],
        [7.760e+00, 2.454e+01, 4.792e+01, ..., 0.000e+00, 2.871e-01,
         7.039e-02]]),
 'target': array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0,
        0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0,
        1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0,
        1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1,
        1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0,
        0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1,
        1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0,
        0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0,
        1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1,
        1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0,
        0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0,
        0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0,
        1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1,
        1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1,
        1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0,
        1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1,
        1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1,
        1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1]),
 'frame': None,
 'target_names': array(['malignant', 'benign'], dtype='<U9'),
 'DESCR': '.. _breast_cancer_dataset:\n\nBreast cancer wisconsin (diagnostic) dataset\n--------------------------------------------\n\n**Data Set Characteristics:**\n\n    :Number of Instances: 569\n\n    :Number of Attributes: 30 numeric, predictive attributes and the class\n\n    :Attribute Information:\n        - radius (mean of distances from center to points on the perimeter)\n        - texture (standard deviation of gray-scale values)\n        - perimeter\n        - area\n        - smoothness (local variation in radius lengths)\n        - compactness (perimeter^2 / area - 1.0)\n        - concavity (severity of concave portions of the contour)\n        - concave points (number of concave portions of the contour)\n        - symmetry\n        - fractal dimension ("coastline approximation" - 1)\n\n        The mean, standard error, and "worst" or largest (mean of the three\n        worst/largest values) of these features were computed for each image,\n        resulting in 30 features.  For instance, field 0 is Mean Radius, field\n        10 is Radius SE, field 20 is Worst Radius.\n\n        - class:\n                - WDBC-Malignant\n                - WDBC-Benign\n\n    :Summary Statistics:\n\n    ===================================== ====== ======\n                                           Min    Max\n    ===================================== ====== ======\n    radius (mean):                        6.981  28.11\n    texture (mean):                       9.71   39.28\n    perimeter (mean):                     43.79  188.5\n    area (mean):                          143.5  2501.0\n    smoothness (mean):                    0.053  0.163\n    compactness (mean):                   0.019  0.345\n    concavity (mean):                     0.0    0.427\n    concave points (mean):                0.0    0.201\n    symmetry (mean):                      0.106  0.304\n    fractal dimension (mean):             0.05   0.097\n    radius (standard error):              0.112  2.873\n    texture (standard error):             0.36   4.885\n    perimeter (standard error):           0.757  21.98\n    area (standard error):                6.802  542.2\n    smoothness (standard error):          0.002  0.031\n    compactness (standard error):         0.002  0.135\n    concavity (standard error):           0.0    0.396\n    concave points (standard error):      0.0    0.053\n    symmetry (standard error):            0.008  0.079\n    fractal dimension (standard error):   0.001  0.03\n    radius (worst):                       7.93   36.04\n    texture (worst):                      12.02  49.54\n    perimeter (worst):                    50.41  251.2\n    area (worst):                         185.2  4254.0\n    smoothness (worst):                   0.071  0.223\n    compactness (worst):                  0.027  1.058\n    concavity (worst):                    0.0    1.252\n    concave points (worst):               0.0    0.291\n    symmetry (worst):                     0.156  0.664\n    fractal dimension (worst):            0.055  0.208\n    ===================================== ====== ======\n\n    :Missing Attribute Values: None\n\n    :Class Distribution: 212 - Malignant, 357 - Benign\n\n    :Creator:  Dr. William H. Wolberg, W. Nick Street, Olvi L. Mangasarian\n\n    :Donor: Nick Street\n\n    :Date: November, 1995\n\nThis is a copy of UCI ML Breast Cancer Wisconsin (Diagnostic) datasets.\nhttps://goo.gl/U2Uwz2\n\nFeatures are computed from a digitized image of a fine needle\naspirate (FNA) of a breast mass.  They describe\ncharacteristics of the cell nuclei present in the image.\n\nSeparating plane described above was obtained using\nMultisurface Method-Tree (MSM-T) [K. P. Bennett, "Decision Tree\nConstruction Via Linear Programming." Proceedings of the 4th\nMidwest Artificial Intelligence and Cognitive Science Society,\npp. 97-101, 1992], a classification method which uses linear\nprogramming to construct a decision tree.  Relevant features\nwere selected using an exhaustive search in the space of 1-4\nfeatures and 1-3 separating planes.\n\nThe actual linear program used to obtain the separating plane\nin the 3-dimensional space is that described in:\n[K. P. Bennett and O. L. Mangasarian: "Robust Linear\nProgramming Discrimination of Two Linearly Inseparable Sets",\nOptimization Methods and Software 1, 1992, 23-34].\n\nThis database is also available through the UW CS ftp server:\n\nftp ftp.cs.wisc.edu\ncd math-prog/cpo-dataset/machine-learn/WDBC/\n\n.. topic:: References\n\n   - W.N. Street, W.H. Wolberg and O.L. Mangasarian. Nuclear feature extraction \n     for breast tumor diagnosis. IS&T/SPIE 1993 International Symposium on \n     Electronic Imaging: Science and Technology, volume 1905, pages 861-870,\n     San Jose, CA, 1993.\n   - O.L. Mangasarian, W.N. Street and W.H. Wolberg. Breast cancer diagnosis and \n     prognosis via linear programming. Operations Research, 43(4), pages 570-577, \n     July-August 1995.\n   - W.H. Wolberg, W.N. Street, and O.L. Mangasarian. Machine learning techniques\n     to diagnose breast cancer from fine-needle aspirates. Cancer Letters 77 (1994) \n     163-171.',
 'feature_names': array(['mean radius', 'mean texture', 'mean perimeter', 'mean area',
        'mean smoothness', 'mean compactness', 'mean concavity',
        'mean concave points', 'mean symmetry', 'mean fractal dimension',
        'radius error', 'texture error', 'perimeter error', 'area error',
        'smoothness error', 'compactness error', 'concavity error',
        'concave points error', 'symmetry error',
        'fractal dimension error', 'worst radius', 'worst texture',
        'worst perimeter', 'worst area', 'worst smoothness',
        'worst compactness', 'worst concavity', 'worst concave points',
        'worst symmetry', 'worst fractal dimension'], dtype='<U23'),
 'filename': 'C:\\Users\\Henri\\anaconda3\\lib\\site-packages\\sklearn\\datasets\\data\\breast_cancer.csv'}
In [185]:
dados.feature_names
Out[185]:
array(['mean radius', 'mean texture', 'mean perimeter', 'mean area',
       'mean smoothness', 'mean compactness', 'mean concavity',
       'mean concave points', 'mean symmetry', 'mean fractal dimension',
       'radius error', 'texture error', 'perimeter error', 'area error',
       'smoothness error', 'compactness error', 'concavity error',
       'concave points error', 'symmetry error',
       'fractal dimension error', 'worst radius', 'worst texture',
       'worst perimeter', 'worst area', 'worst smoothness',
       'worst compactness', 'worst concavity', 'worst concave points',
       'worst symmetry', 'worst fractal dimension'], dtype='<U23')
In [195]:
from sklearn.model_selection import train_test_split

entrada_treino, entrada_teste, saida_treino, saida_teste = train_test_split(dados.data,dados.target,test_size=0.2)
In [197]:
len(saida_treino)
Out[197]:
455
In [198]:
len(saida_teste)
Out[198]:
114
Treino e predição
Árvores de decisões
In [200]:
from sklearn.tree import DecisionTreeClassifier

classificador = DecisionTreeClassifier()

classificador.fit(entrada_treino,saida_treino)
Out[200]:
DecisionTreeClassifier()
In [202]:
entrada_teste
Out[202]:
array([[2.742e+01, 2.627e+01, 1.869e+02, ..., 2.625e-01, 2.641e-01,
        7.427e-02],
       [1.287e+01, 1.621e+01, 8.238e+01, ..., 5.780e-02, 3.604e-01,
        7.062e-02],
       [1.665e+01, 2.138e+01, 1.100e+02, ..., 2.095e-01, 3.613e-01,
        9.564e-02],
       ...,
       [1.561e+01, 1.938e+01, 1.000e+02, ..., 8.568e-02, 2.683e-01,
        6.829e-02],
       [1.405e+01, 2.715e+01, 9.138e+01, ..., 1.048e-01, 2.250e-01,
        8.321e-02],
       [1.132e+01, 2.708e+01, 7.176e+01, ..., 2.083e-02, 2.849e-01,
        7.087e-02]])
In [205]:
saida_obtida = classificador.predict(entrada_teste)

saida_obtida
Out[205]:
array([0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0,
       1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1,
       0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0,
       0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0,
       1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0,
       0, 0, 0, 0])
In [203]:
saida_teste
Out[203]:
array([0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0,
       1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1,
       0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0,
       0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0,
       1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0,
       1, 0, 1, 1])
In [206]:
saida_obtida == saida_teste
Out[206]:
array([ True,  True,  True,  True,  True,  True,  True,  True,  True,
        True,  True,  True,  True,  True,  True,  True,  True, False,
        True,  True,  True,  True,  True,  True,  True,  True,  True,
        True,  True,  True, False,  True,  True,  True,  True,  True,
        True,  True,  True,  True,  True,  True,  True,  True,  True,
       False,  True,  True,  True,  True,  True,  True,  True,  True,
        True,  True,  True,  True,  True,  True,  True, False,  True,
        True,  True,  True,  True,  True, False,  True,  True,  True,
        True,  True,  True,  True, False,  True,  True,  True,  True,
        True,  True,  True,  True, False,  True,  True,  True,  True,
        True,  True,  True,  True, False,  True,  True,  True,  True,
        True,  True, False,  True,  True,  True,  True,  True,  True,
        True,  True, False,  True, False, False])
In [207]:
(saida_obtida != saida_teste).sum()
Out[207]:
12
In [208]:
len(saida_obtida)
Out[208]:
114
In [210]:
classificador.score(entrada_teste,saida_teste)*100
Out[210]:
89.47368421052632
In [211]:
(saida_obtida == saida_teste).sum()*100/len(saida_obtida)
Out[211]:
89.47368421052632
In [212]:
from sklearn.metrics import confusion_matrix

confusion_matrix(saida_teste,saida_obtida)
Out[212]:
array([[38,  4],
       [ 8, 64]], dtype=int64)
Máquinas de Vetor de Suporte
In [225]:
from sklearn import svm

classificador = svm.SVC()

classificador.fit(entrada_treino,saida_treino)
Out[225]:
SVC()
In [226]:
saida_obtida = classificador.predict(entrada_teste)

saida_obtida
Out[226]:
array([0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0,
       1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1,
       0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0,
       0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0,
       1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0,
       1, 0, 1, 1])
In [227]:
saida_obtida == saida_teste
Out[227]:
array([ True,  True,  True,  True,  True,  True,  True,  True,  True,
        True,  True,  True,  True,  True,  True,  True,  True,  True,
        True,  True,  True,  True,  True,  True, False,  True,  True,
        True,  True,  True,  True, False,  True,  True,  True,  True,
        True,  True,  True,  True,  True,  True,  True,  True,  True,
        True,  True,  True,  True,  True,  True,  True,  True,  True,
        True,  True,  True,  True,  True,  True,  True,  True, False,
        True,  True,  True,  True,  True,  True,  True,  True, False,
        True,  True,  True,  True, False,  True,  True,  True,  True,
        True,  True,  True,  True, False, False,  True,  True,  True,
        True,  True,  True,  True,  True,  True,  True,  True,  True,
        True,  True,  True,  True,  True, False,  True, False,  True,
        True,  True,  True,  True,  True,  True])
In [228]:
(saida_obtida != saida_teste).sum()
Out[228]:
9
In [230]:
classificador.score(entrada_teste,saida_teste)*100
Out[230]:
92.10526315789474
In [232]:
from sklearn.metrics import confusion_matrix

confusion_matrix(saida_teste,saida_obtida)
Out[232]:
array([[34,  8],
       [ 1, 71]], dtype=int64)
Gradientes Descendentes Estocásticos
In [233]:
from sklearn.linear_model import SGDClassifier

classificador = SGDClassifier()

classificador.fit(entrada_treino,saida_treino)
Out[233]:
SGDClassifier()
In [234]:
saida_obtida = classificador.predict(entrada_teste)

saida_obtida
Out[234]:
array([0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0,
       1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1,
       0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0,
       0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0,
       1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1,
       1, 1, 1, 1])
In [235]:
saida_obtida == saida_teste
Out[235]:
array([ True,  True,  True,  True,  True, False,  True,  True,  True,
        True,  True,  True,  True,  True,  True,  True,  True,  True,
        True,  True,  True,  True,  True,  True, False,  True,  True,
        True,  True,  True,  True, False,  True,  True,  True,  True,
        True,  True,  True,  True,  True,  True,  True,  True,  True,
       False,  True,  True,  True,  True,  True,  True,  True,  True,
        True,  True, False,  True,  True,  True,  True,  True, False,
        True,  True,  True,  True,  True,  True,  True,  True, False,
        True,  True,  True,  True, False,  True,  True,  True,  True,
        True,  True,  True,  True, False, False,  True,  True,  True,
        True,  True,  True,  True,  True,  True,  True, False,  True,
        True,  True,  True,  True,  True, False,  True, False,  True,
        True, False,  True, False,  True,  True])
In [236]:
(saida_obtida != saida_teste).sum()
Out[236]:
15
In [237]:
classificador.score(entrada_teste,saida_teste)*100
Out[237]:
86.8421052631579
In [238]:
from sklearn.metrics import confusion_matrix

confusion_matrix(saida_teste,saida_obtida)
Out[238]:
array([[30, 12],
       [ 3, 69]], dtype=int64)
Iris Dataset
In [213]:
iris = datasets.load_iris()

iris
Out[213]:
{'data': array([[5.1, 3.5, 1.4, 0.2],
        [4.9, 3. , 1.4, 0.2],
        [4.7, 3.2, 1.3, 0.2],
        [4.6, 3.1, 1.5, 0.2],
        [5. , 3.6, 1.4, 0.2],
        [5.4, 3.9, 1.7, 0.4],
        [4.6, 3.4, 1.4, 0.3],
        [5. , 3.4, 1.5, 0.2],
        [4.4, 2.9, 1.4, 0.2],
        [4.9, 3.1, 1.5, 0.1],
        [5.4, 3.7, 1.5, 0.2],
        [4.8, 3.4, 1.6, 0.2],
        [4.8, 3. , 1.4, 0.1],
        [4.3, 3. , 1.1, 0.1],
        [5.8, 4. , 1.2, 0.2],
        [5.7, 4.4, 1.5, 0.4],
        [5.4, 3.9, 1.3, 0.4],
        [5.1, 3.5, 1.4, 0.3],
        [5.7, 3.8, 1.7, 0.3],
        [5.1, 3.8, 1.5, 0.3],
        [5.4, 3.4, 1.7, 0.2],
        [5.1, 3.7, 1.5, 0.4],
        [4.6, 3.6, 1. , 0.2],
        [5.1, 3.3, 1.7, 0.5],
        [4.8, 3.4, 1.9, 0.2],
        [5. , 3. , 1.6, 0.2],
        [5. , 3.4, 1.6, 0.4],
        [5.2, 3.5, 1.5, 0.2],
        [5.2, 3.4, 1.4, 0.2],
        [4.7, 3.2, 1.6, 0.2],
        [4.8, 3.1, 1.6, 0.2],
        [5.4, 3.4, 1.5, 0.4],
        [5.2, 4.1, 1.5, 0.1],
        [5.5, 4.2, 1.4, 0.2],
        [4.9, 3.1, 1.5, 0.2],
        [5. , 3.2, 1.2, 0.2],
        [5.5, 3.5, 1.3, 0.2],
        [4.9, 3.6, 1.4, 0.1],
        [4.4, 3. , 1.3, 0.2],
        [5.1, 3.4, 1.5, 0.2],
        [5. , 3.5, 1.3, 0.3],
        [4.5, 2.3, 1.3, 0.3],
        [4.4, 3.2, 1.3, 0.2],
        [5. , 3.5, 1.6, 0.6],
        [5.1, 3.8, 1.9, 0.4],
        [4.8, 3. , 1.4, 0.3],
        [5.1, 3.8, 1.6, 0.2],
        [4.6, 3.2, 1.4, 0.2],
        [5.3, 3.7, 1.5, 0.2],
        [5. , 3.3, 1.4, 0.2],
        [7. , 3.2, 4.7, 1.4],
        [6.4, 3.2, 4.5, 1.5],
        [6.9, 3.1, 4.9, 1.5],
        [5.5, 2.3, 4. , 1.3],
        [6.5, 2.8, 4.6, 1.5],
        [5.7, 2.8, 4.5, 1.3],
        [6.3, 3.3, 4.7, 1.6],
        [4.9, 2.4, 3.3, 1. ],
        [6.6, 2.9, 4.6, 1.3],
        [5.2, 2.7, 3.9, 1.4],
        [5. , 2. , 3.5, 1. ],
        [5.9, 3. , 4.2, 1.5],
        [6. , 2.2, 4. , 1. ],
        [6.1, 2.9, 4.7, 1.4],
        [5.6, 2.9, 3.6, 1.3],
        [6.7, 3.1, 4.4, 1.4],
        [5.6, 3. , 4.5, 1.5],
        [5.8, 2.7, 4.1, 1. ],
        [6.2, 2.2, 4.5, 1.5],
        [5.6, 2.5, 3.9, 1.1],
        [5.9, 3.2, 4.8, 1.8],
        [6.1, 2.8, 4. , 1.3],
        [6.3, 2.5, 4.9, 1.5],
        [6.1, 2.8, 4.7, 1.2],
        [6.4, 2.9, 4.3, 1.3],
        [6.6, 3. , 4.4, 1.4],
        [6.8, 2.8, 4.8, 1.4],
        [6.7, 3. , 5. , 1.7],
        [6. , 2.9, 4.5, 1.5],
        [5.7, 2.6, 3.5, 1. ],
        [5.5, 2.4, 3.8, 1.1],
        [5.5, 2.4, 3.7, 1. ],
        [5.8, 2.7, 3.9, 1.2],
        [6. , 2.7, 5.1, 1.6],
        [5.4, 3. , 4.5, 1.5],
        [6. , 3.4, 4.5, 1.6],
        [6.7, 3.1, 4.7, 1.5],
        [6.3, 2.3, 4.4, 1.3],
        [5.6, 3. , 4.1, 1.3],
        [5.5, 2.5, 4. , 1.3],
        [5.5, 2.6, 4.4, 1.2],
        [6.1, 3. , 4.6, 1.4],
        [5.8, 2.6, 4. , 1.2],
        [5. , 2.3, 3.3, 1. ],
        [5.6, 2.7, 4.2, 1.3],
        [5.7, 3. , 4.2, 1.2],
        [5.7, 2.9, 4.2, 1.3],
        [6.2, 2.9, 4.3, 1.3],
        [5.1, 2.5, 3. , 1.1],
        [5.7, 2.8, 4.1, 1.3],
        [6.3, 3.3, 6. , 2.5],
        [5.8, 2.7, 5.1, 1.9],
        [7.1, 3. , 5.9, 2.1],
        [6.3, 2.9, 5.6, 1.8],
        [6.5, 3. , 5.8, 2.2],
        [7.6, 3. , 6.6, 2.1],
        [4.9, 2.5, 4.5, 1.7],
        [7.3, 2.9, 6.3, 1.8],
        [6.7, 2.5, 5.8, 1.8],
        [7.2, 3.6, 6.1, 2.5],
        [6.5, 3.2, 5.1, 2. ],
        [6.4, 2.7, 5.3, 1.9],
        [6.8, 3. , 5.5, 2.1],
        [5.7, 2.5, 5. , 2. ],
        [5.8, 2.8, 5.1, 2.4],
        [6.4, 3.2, 5.3, 2.3],
        [6.5, 3. , 5.5, 1.8],
        [7.7, 3.8, 6.7, 2.2],
        [7.7, 2.6, 6.9, 2.3],
        [6. , 2.2, 5. , 1.5],
        [6.9, 3.2, 5.7, 2.3],
        [5.6, 2.8, 4.9, 2. ],
        [7.7, 2.8, 6.7, 2. ],
        [6.3, 2.7, 4.9, 1.8],
        [6.7, 3.3, 5.7, 2.1],
        [7.2, 3.2, 6. , 1.8],
        [6.2, 2.8, 4.8, 1.8],
        [6.1, 3. , 4.9, 1.8],
        [6.4, 2.8, 5.6, 2.1],
        [7.2, 3. , 5.8, 1.6],
        [7.4, 2.8, 6.1, 1.9],
        [7.9, 3.8, 6.4, 2. ],
        [6.4, 2.8, 5.6, 2.2],
        [6.3, 2.8, 5.1, 1.5],
        [6.1, 2.6, 5.6, 1.4],
        [7.7, 3. , 6.1, 2.3],
        [6.3, 3.4, 5.6, 2.4],
        [6.4, 3.1, 5.5, 1.8],
        [6. , 3. , 4.8, 1.8],
        [6.9, 3.1, 5.4, 2.1],
        [6.7, 3.1, 5.6, 2.4],
        [6.9, 3.1, 5.1, 2.3],
        [5.8, 2.7, 5.1, 1.9],
        [6.8, 3.2, 5.9, 2.3],
        [6.7, 3.3, 5.7, 2.5],
        [6.7, 3. , 5.2, 2.3],
        [6.3, 2.5, 5. , 1.9],
        [6.5, 3. , 5.2, 2. ],
        [6.2, 3.4, 5.4, 2.3],
        [5.9, 3. , 5.1, 1.8]]),
 'target': array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,
        2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,
        2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]),
 'frame': None,
 'target_names': array(['setosa', 'versicolor', 'virginica'], dtype='<U10'),
 'DESCR': '.. _iris_dataset:\n\nIris plants dataset\n--------------------\n\n**Data Set Characteristics:**\n\n    :Number of Instances: 150 (50 in each of three classes)\n    :Number of Attributes: 4 numeric, predictive attributes and the class\n    :Attribute Information:\n        - sepal length in cm\n        - sepal width in cm\n        - petal length in cm\n        - petal width in cm\n        - class:\n                - Iris-Setosa\n                - Iris-Versicolour\n                - Iris-Virginica\n                \n    :Summary Statistics:\n\n    ============== ==== ==== ======= ===== ====================\n                    Min  Max   Mean    SD   Class Correlation\n    ============== ==== ==== ======= ===== ====================\n    sepal length:   4.3  7.9   5.84   0.83    0.7826\n    sepal width:    2.0  4.4   3.05   0.43   -0.4194\n    petal length:   1.0  6.9   3.76   1.76    0.9490  (high!)\n    petal width:    0.1  2.5   1.20   0.76    0.9565  (high!)\n    ============== ==== ==== ======= ===== ====================\n\n    :Missing Attribute Values: None\n    :Class Distribution: 33.3% for each of 3 classes.\n    :Creator: R.A. Fisher\n    :Donor: Michael Marshall (MARSHALL%PLU@io.arc.nasa.gov)\n    :Date: July, 1988\n\nThe famous Iris database, first used by Sir R.A. Fisher. The dataset is taken\nfrom Fisher\'s paper. Note that it\'s the same as in R, but not as in the UCI\nMachine Learning Repository, which has two wrong data points.\n\nThis is perhaps the best known database to be found in the\npattern recognition literature.  Fisher\'s paper is a classic in the field and\nis referenced frequently to this day.  (See Duda & Hart, for example.)  The\ndata set contains 3 classes of 50 instances each, where each class refers to a\ntype of iris plant.  One class is linearly separable from the other 2; the\nlatter are NOT linearly separable from each other.\n\n.. topic:: References\n\n   - Fisher, R.A. "The use of multiple measurements in taxonomic problems"\n     Annual Eugenics, 7, Part II, 179-188 (1936); also in "Contributions to\n     Mathematical Statistics" (John Wiley, NY, 1950).\n   - Duda, R.O., & Hart, P.E. (1973) Pattern Classification and Scene Analysis.\n     (Q327.D83) John Wiley & Sons.  ISBN 0-471-22361-1.  See page 218.\n   - Dasarathy, B.V. (1980) "Nosing Around the Neighborhood: A New System\n     Structure and Classification Rule for Recognition in Partially Exposed\n     Environments".  IEEE Transactions on Pattern Analysis and Machine\n     Intelligence, Vol. PAMI-2, No. 1, 67-71.\n   - Gates, G.W. (1972) "The Reduced Nearest Neighbor Rule".  IEEE Transactions\n     on Information Theory, May 1972, 431-433.\n   - See also: 1988 MLC Proceedings, 54-64.  Cheeseman et al"s AUTOCLASS II\n     conceptual clustering system finds 3 classes in the data.\n   - Many, many more ...',
 'feature_names': ['sepal length (cm)',
  'sepal width (cm)',
  'petal length (cm)',
  'petal width (cm)'],
 'filename': 'C:\\Users\\Henri\\anaconda3\\lib\\site-packages\\sklearn\\datasets\\data\\iris.csv'}
In [214]:
iris.feature_names
Out[214]:
['sepal length (cm)',
 'sepal width (cm)',
 'petal length (cm)',
 'petal width (cm)']
In [217]:
a_treino, a_teste, b_treino, b_teste = train_test_split(iris.data, iris.target)

classificador_iris = DecisionTreeClassifier()

classificador_iris.fit(a_treino,b_treino)
Out[217]:
DecisionTreeClassifier()
In [222]:
predição_iris = classificador_iris.predict(a_teste)

predição_iris

[iris.target_names[i] for i in predição_iris]
Out[222]:
['versicolor',
 'setosa',
 'virginica',
 'versicolor',
 'setosa',
 'virginica',
 'setosa',
 'virginica',
 'versicolor',
 'setosa',
 'setosa',
 'virginica',
 'versicolor',
 'setosa',
 'virginica',
 'versicolor',
 'setosa',
 'virginica',
 'setosa',
 'setosa',
 'setosa',
 'setosa',
 'setosa',
 'setosa',
 'versicolor',
 'setosa',
 'versicolor',
 'virginica',
 'setosa',
 'virginica',
 'setosa',
 'versicolor',
 'setosa',
 'versicolor',
 'versicolor',
 'setosa',
 'virginica',
 'virginica']
In [223]:
confusion_matrix(b_teste,predição_iris)
Out[223]:
array([[18,  0,  0],
       [ 0,  9,  1],
       [ 0,  1,  9]], dtype=int64)
In [224]:
iris.target_names
Out[224]:
array(['setosa', 'versicolor', 'virginica'], dtype='<U10')
Clusterização
In [269]:
from sklearn.datasets import make_blobs

a, b = make_blobs(n_samples = 50, n_features = 2, centers = 2)

a
Out[269]:
array([[-1.5404508 ,  4.6632082 ],
       [-2.82553165,  5.02057065],
       [-4.3674585 ,  7.05259857],
       [-2.53971588,  3.07792269],
       [-0.98389175,  4.17206281],
       [-1.53676755,  3.55364622],
       [-5.64105856,  7.88008532],
       [-0.37746692,  5.26952429],
       [-5.8196035 ,  5.70475311],
       [-1.91176037,  2.49276399],
       [-5.41417323,  7.44052725],
       [-4.81840366,  7.2062582 ],
       [-2.19637847,  4.44053057],
       [-0.95887966,  3.66203557],
       [-0.97404281,  3.20275095],
       [-4.0443133 ,  5.54432799],
       [-0.62081857,  4.17409121],
       [-3.56006028,  7.36139782],
       [-7.36210488,  7.41262733],
       [-4.61880279,  6.24355681],
       [-2.64917786,  3.68307203],
       [-1.64836079,  5.40353396],
       [-5.62773033,  6.14682176],
       [-5.69930467,  5.63882636],
       [-5.06777622,  8.54609463],
       [-4.35732679,  5.82420715],
       [-2.58790211,  4.97855488],
       [-2.802097  ,  2.07169563],
       [-4.54164611,  7.41351976],
       [-1.85625564,  3.54019211],
       [-4.53174918,  7.56548253],
       [-1.84687695,  2.79706775],
       [-2.09878681,  4.7887888 ],
       [-4.97749574,  5.19926535],
       [-5.92966381,  8.92571216],
       [-1.15258198,  5.59475384],
       [-0.87417605,  2.27441989],
       [-6.62293392,  8.94944087],
       [-6.01180518,  4.97403286],
       [-3.81536755,  5.48169014],
       [-6.17390852,  6.02775612],
       [-4.35560154,  6.31912589],
       [-0.9651864 ,  4.63582566],
       [-4.92480983,  6.37386241],
       [-2.3786332 ,  2.23033553],
       [-3.68399319,  7.40932009],
       [-5.05449794,  6.74483832],
       [-7.42721735,  7.1857435 ],
       [-1.05718314,  3.14099484],
       [-0.55001937,  4.54578752]])
In [270]:
plt.scatter(a[:,0],a[:,1])
Out[270]:
<matplotlib.collections.PathCollection at 0x2b4a9e01d60>

K-Means
In [271]:
from sklearn import cluster

clusterizador = cluster.KMeans(n_clusters=2)
In [272]:
solução = clusterizador.fit_predict(a)
In [273]:
solução
Out[273]:
array([1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1,
       0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0,
       1, 0, 0, 0, 1, 1])
In [274]:
plt.grid(zorder=0)
plt.scatter(a[solução==0,0],a[solução==0,1],c="red",s=100,zorder=2,marker="s",edgecolor="black",label="cluster 1")
plt.scatter(a[solução==1,0],a[solução==1,1],c="blue",s=100,zorder=2,marker="*",edgecolor="black",label="cluster 2")
plt.legend()
Out[274]:
<matplotlib.legend.Legend at 0x2b4a9d3cfa0>

Mean Shift
In [275]:
from sklearn import cluster

clusterizador = cluster.MeanShift()
In [276]:
solução = clusterizador.fit_predict(a)
In [277]:
solução
Out[277]:
array([0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0,
       1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1,
       0, 1, 1, 1, 0, 0], dtype=int64)
In [278]:
plt.grid(zorder=0)
plt.scatter(a[solução==0,0],a[solução==0,1],c="red",s=100,zorder=2,marker="s",edgecolor="black",label="cluster 1")
plt.scatter(a[solução==1,0],a[solução==1,1],c="blue",s=100,zorder=2,marker="*",edgecolor="black",label="cluster 2")
plt.legend()
Out[278]:
<matplotlib.legend.Legend at 0x2b4a9eb9d60>

Regressão
In [285]:
from numpy.random import rand
a = rand(10,1)
a
Out[285]:
array([[0.78604405],
       [0.27114734],
       [0.10047188],
       [0.25627138],
       [0.71552522],
       [0.15533144],
       [0.35258742],
       [0.08821561],
       [0.56886686],
       [0.438268  ]])
In [294]:
b = c+a.transpose()
b
Out[294]:
array([[0.78604405, 0.38225845, 0.32269411, 0.58960472, 1.15996966,
        0.710887  , 1.01925408, 0.86599338, 1.45775575, 1.438268  ]])
In [287]:
from numpy import linspace
c = linspace(0,1,10)
c
Out[287]:
array([0.        , 0.11111111, 0.22222222, 0.33333333, 0.44444444,
       0.55555556, 0.66666667, 0.77777778, 0.88888889, 1.        ])
In [296]:
x = c
y = b

plt.scatter(c,b)
Out[296]:
<matplotlib.collections.PathCollection at 0x2b4ab5b8490>

Regressão Linear
In [322]:
import numpy as np

z = np.zeros([1,10])
z[:] = x
x = z.transpose()
In [316]:
x
Out[316]:
array([0.        , 0.11111111, 0.22222222, 0.33333333, 0.44444444,
       0.55555556, 0.66666667, 0.77777778, 0.88888889, 1.        ])
In [323]:
import numpy as np

z = np.zeros([1,10])
z[:] = y
y = z.transpose()
In [324]:
y
Out[324]:
array([[0.78604405],
       [0.38225845],
       [0.32269411],
       [0.58960472],
       [1.15996966],
       [0.710887  ],
       [1.01925408],
       [0.86599338],
       [1.45775575],
       [1.438268  ]])
In [325]:
from sklearn.linear_model import LinearRegression

reg = LinearRegression()

reg.fit(x,y)
Out[325]:
LinearRegression()
In [327]:
c = linspace(0,1,100)
d = reg.predict(np.matrix(c).T)
In [328]:
plt.plot(x,y,'o',c,d)
Out[328]:
[<matplotlib.lines.Line2D at 0x2b4ab629610>,
 <matplotlib.lines.Line2D at 0x2b4ab629490>]

In [329]:
from sklearn import neighbors

reg2 = neighbors.KNeighborsRegressor()

reg2.fit(x,y)
Out[329]:
KNeighborsRegressor()
In [333]:
c = linspace(0,1,100)
d = reg2.predict(np.matrix(c).T)
In [334]:
plt.plot(x,y,'o',c,d)
Out[334]:
[<matplotlib.lines.Line2D at 0x2b4ab723eb0>,
 <matplotlib.lines.Line2D at 0x2b4ab7233a0>]

Lidando com texto
In [383]:
categories = ["alt.atheism","soc.religion.christian","comp.graphics","sci.med"]

from sklearn.datasets import fetch_20newsgroups

t_treino = fetch_20newsgroups(subset="train",
                             categories=categories,
                             shuffle=True,
                             random_state=42)
Bags of Words
In [343]:
from sklearn.feature_extraction.text import CountVectorizer

contador = CountVectorizer()
contagem_treino = contador.fit_transform(t_treino.data)
contagem_treino
Out[343]:
<2257x35788 sparse matrix of type '<class 'numpy.int64'>'
	with 365886 stored elements in Compressed Sparse Row format>
In [346]:
contagem_treino.shape
Out[346]:
(2257, 35788)
TF-IDF
Term Frequence - calcular porcentagem
Inverse Document Frequence - ignorar palavras comuns
In [351]:
from sklearn.feature_extraction.text import TfidfTransformer

tfidf_transformer = TfidfTransformer()

contagem_tfidf = tfidf_transformer.fit_transform(contagem_treino)

contagem_tfidf.shape
Out[351]:
(2257, 35788)
Classificar o texto
In [353]:
from sklearn.linear_model import SGDClassifier

classificador_t = SGDClassifier(loss="hinge",penalty="l2",
                             alpha=1e-3, random_state=42,
                             max_iter = 5, tol=None)

classificador_t.fit(contagem_tfidf, t_treino.target)
Out[353]:
SGDClassifier(alpha=0.001, max_iter=5, random_state=42, tol=None)
In [362]:
novos_textos = ['God is love','OpenGL on GPU is fast']

contagem_teste = contador.transform(novos_textos)

tfidf_transformer2 = TfidfTransformer()
contagem_tfidf_teste = tfidf_transformer2.fit_transform(contagem_teste)
In [363]:
resultado = classificador_t.predict(contagem_tfidf_teste)
In [364]:
resultado
Out[364]:
array([3, 1], dtype=int64)
In [365]:
t_treino.target_names
Out[365]:
['alt.atheism', 'comp.graphics', 'sci.med', 'soc.religion.christian']
Subset de testagem
In [385]:
t_teste = fetch_20newsgroups(subset="test",
                             categories=categories,
                             shuffle=True,
                             random_state=42)
In [367]:
contagem_teste = contador.transform(t_teste.data)
contagem_teste
Out[367]:
<1502x35788 sparse matrix of type '<class 'numpy.int64'>'
	with 241899 stored elements in Compressed Sparse Row format>
In [369]:
contagem_teste_tfidf = tfidf_transformer.transform(contagem_teste)
In [370]:
solução_teste = classificador_t.predict(contagem_teste_tfidf)
In [390]:
(solução_teste != t_teste.target).sum()
Out[390]:
135
In [378]:
classificador_t.score(contagem_teste_tfidf,t_teste.target)
Out[378]:
0.9101198402130493
Pipelines
In [381]:
from sklearn.pipeline import Pipeline
Pipeline(...)
Lista
Tuplas
Texto
Transformador/classificador
In [382]:
clf_pipeline = Pipeline([
    ('vect',CountVectorizer()),
    ('tfidf',TfidfTransformer()),
    ('clf',SGDClassifier(loss="hinge",penalty="l2",
                             alpha=1e-3, random_state=42,
                             max_iter = 5, tol=None))
])
In [384]:
clf_pipeline.fit(t_treino.data,t_treino.target)
Out[384]:
Pipeline(steps=[('vect', CountVectorizer()), ('tfidf', TfidfTransformer()),
                ('clf',
                 SGDClassifier(alpha=0.001, max_iter=5, random_state=42,
                               tol=None))])
In [387]:
sol_pipeline = clf_pipeline.predict(t_teste.data)
In [389]:
(sol_pipeline != t_teste.target).sum()
Out[389]:
135
In [391]:
clf_pipeline.score(t_teste.data,t_teste.target)
Out[391]:
0.9101198402130493
Hash
Não pode ser invertido
f(x) = 2x
Inversível
g(x) = soma dos algarismos de x
Não-inversível
Não pode ter muito conflito, tais como:
g(13) = 4
g(22) = 4
Caótica, ou seja, deve variar muito, diferentemente de:
g(13) = 4
g(14) = 5
Saídas devem ter o mesmo tamanho, diferentemente de:
g(10) = 1
g(55) = 10
Vamos gravar cada palavra na coluna correspondente ao seu hash, e vamos definir uma quantidade menor de colunas

In [402]:
from sklearn.feature_extraction.text import HashingVectorizer

clf_pipeline_hash = Pipeline([
    ('hash',HashingVectorizer(n_features=20_000)),
    ('tfidf',TfidfTransformer()),
    ('clf',SGDClassifier(loss="hinge",penalty="l2",
                             alpha=1e-3, random_state=42,
                             max_iter = 5, tol=None))
])
In [398]:
clf_pipeline_hash.fit(t_treino.data,t_treino.target)
Out[398]:
Pipeline(steps=[('hash', HashingVectorizer(n_features=20000)),
                ('tfidf', TfidfTransformer()),
                ('clf',
                 SGDClassifier(alpha=0.001, max_iter=5, random_state=42,
                               tol=None))])
In [400]:
clf_pipeline_hash.score(t_teste.data,t_teste.target)
Out[400]:
0.8974700399467377
Vazamento de dados (cuidado!)
In [401]:
clf_pipeline_hash.score(t_treino.data,t_treino.target)
Out[401]:
0.9933540097474524
